{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "beea4031",
   "metadata": {},
   "source": [
    "# Decision Generator (Anchored, Recency-Aware, TAST-Driven, Risk-Capped)\n",
    "\n",
    "This notebook generates the next-round decision files:\n",
    "- `data/prices/prices_<vNext>.json`\n",
    "- `data/amounts/amounts_<vNext>.json`\n",
    "- `data/schedules/schedules_<vNext>.json`\n",
    "\n",
    "It reads `model_data.json` (from the Model & Analysis Generator) and applies:\n",
    "- **Anchor pricing from the best-selling week** (per product)\n",
    "- **TAST_norm** (time-adjusted sell-through) for demand intensity\n",
    "- **Recency decay** (penalize items with long time since last sale)\n",
    "- **Dead-item suppression** (stronger reductions for zero recent sales)\n",
    "- **Risk-capped amounts** (avoid expensive overstock)\n",
    "- **Schedule by ROI** (revenue / staff cost, once per version)\n",
    "\n",
    "Exports an **audit CSV** and prints quick anomaly checks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9cb178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & Parameters ======================================================\n",
    "import json, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path(\"data\")\n",
    "SCHEDULES_DIR = DATA_DIR / \"schedules\"\n",
    "OUT_DIR = Path(\"output\"); OUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "MODEL_DATA_PATH = Path(\"model_data.json\")  # produced by model_and_analysis_generator\n",
    "\n",
    "# Core parameters (conservative for competitive play)\n",
    "PRICE_UP_HOT      = 0.03   # +3% if very hot (TAST high)\n",
    "PRICE_UP_GOOD     = 0.02   # +2% if strong demand\n",
    "PRICE_DOWN_WEAK   = 0.08   # -8% if weak demand with stock\n",
    "PRICE_DOWN_DEAD   = 0.15   # -15% if dead item\n",
    "\n",
    "MIN_MARGIN_RATE   = 0.10   # ensure price >= (1 + MIN_MARGIN_RATE) * supplier_price\n",
    "ROUND_CAP_STD     = 0.05   # ±5% per round cap\n",
    "ROUND_CAP_DEAD    = 0.10   # ±10% for dead items\n",
    "\n",
    "AMT_BASE_BUFFER   = 1.20   # base buffer over max observed weekly demand\n",
    "AMT_BOOST_HOT     = 1.20   # extra +20% for very hot items\n",
    "AMT_DEAD_FACTOR   = 0.50   # dead items keep 50% of average stock\n",
    "RISK_CAP_MIN      = 1.10   # min cap multiplier on avg amount\n",
    "RISK_CAP_MAX      = 1.60   # max cap multiplier on avg amount\n",
    "\n",
    "EMA_ALPHA         = 0.6    # recency emphasis\n",
    "RECENCY_DECAY     = 0.10   # 10% penalty per week since last sale (clamped)\n",
    "RECENCY_MIN       = 0.60   # floor 60%\n",
    "\n",
    "# TAST thresholds (tune if needed)\n",
    "TAST_HOT          = 0.90   # sold out early / very strong\n",
    "TAST_GOOD_LOW     = 0.65   # good demand lower bound\n",
    "TAST_WEAK_HIGH    = 0.25   # weak demand upper bound\n",
    "\n",
    "# Optional: product name filters for quick audits\n",
    "ANOMALY_FILTERS   = [\"dinosaur\", \"mattress\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f850bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Load model data + helpers ==============================================\n",
    "assert MODEL_DATA_PATH.exists(), \"model_data.json not found. Run model_and_analysis_generator first.\"\n",
    "md = pd.DataFrame(json.load(open(MODEL_DATA_PATH, \"r\", encoding=\"utf-8\")))\n",
    "\n",
    "VERSIONS = sorted(md[\"version\"].unique())\n",
    "V_NOW  = VERSIONS[-1]\n",
    "HIST_VERS = [v for v in VERSIONS if v < V_NOW]\n",
    "V_NEXT = V_NOW + 1\n",
    "\n",
    "print(\"Versions:\", VERSIONS, \"| Learn from:\", HIST_VERS, \"| Next:\", V_NEXT)\n",
    "\n",
    "def safe_div(a, b):\n",
    "    a = np.asarray(a, float); b = np.asarray(b, float)\n",
    "    return np.divide(a, b, out=np.zeros_like(a), where=b!=0)\n",
    "\n",
    "def ema_series(x, alpha=EMA_ALPHA):\n",
    "    s = None\n",
    "    for xi in x:\n",
    "        s = alpha*xi + (1-alpha)*(s if s is not None else xi)\n",
    "    return s if s is not None else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad31242",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Build historical table & select anchors =================================\n",
    "# Aggregate core facts per (version, product)\n",
    "g = (md.groupby([\"version\",\"product\"], as_index=False)\n",
    "        .agg(qty=(\"qty\",\"sum\"),\n",
    "             amount=(\"amount\",\"mean\"),\n",
    "             price=(\"price\",\"mean\"),\n",
    "             revenue=(\"revenue\",\"sum\"),\n",
    "             supplier_price=(\"supplier_price\",\"mean\"),\n",
    "             TAST_norm=(\"TAST_norm\",\"mean\")))  # TAST_norm now available\n",
    "\n",
    "g_hist = g[g[\"version\"].isin(HIST_VERS)].copy()\n",
    "\n",
    "# Anchor: best-selling week by qty, then higher revenue, then lower price\n",
    "g_sorted = g_hist.sort_values([\"product\",\"qty\",\"revenue\",\"price\"],\n",
    "                              ascending=[True, False, False, True])\n",
    "anchor = (g_sorted.groupby(\"product\", as_index=False).first()\n",
    "                 [[\"product\",\"version\",\"price\",\"qty\",\"revenue\",\"amount\",\"supplier_price\"]]\n",
    "                 .rename(columns={\"version\":\"anchor_v\",\"price\":\"p_anchor\",\n",
    "                                  \"qty\":\"anchor_qty\",\"revenue\":\"anchor_revenue\",\n",
    "                                  \"amount\":\"anchor_amount\",\n",
    "                                  \"supplier_price\":\"supplier_price_ref\"}))\n",
    "\n",
    "print(\"Anchor sample:\")\n",
    "display(anchor.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9215123e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Indicators: TAST-EMA, VOL, Risk, Recency ================================\n",
    "# Per-product historical indicators\n",
    "hist = (g_hist.groupby(\"product\")\n",
    "          .apply(lambda d: pd.Series({\n",
    "              # Use TAST_norm (time-aware sell-through) as the demand signal\n",
    "              \"TAST_EMA\": ema_series(d[\"TAST_norm\"].fillna(0.0), alpha=EMA_ALPHA),\n",
    "              \"qty_max\": d[\"qty\"].max(),\n",
    "              \"amount_avg\": d[\"amount\"].mean(),\n",
    "              \"VOL\": float(d[\"qty\"].std(ddof=0)) if len(d) > 1 else 0.0\n",
    "          }))\n",
    "          .reset_index())\n",
    "\n",
    "# Medians for risk scaling\n",
    "p_med = g_hist.groupby(\"product\")[\"price\"].median().rename(\"price_med\")\n",
    "c_med = g_hist.groupby(\"product\")[\"supplier_price\"].median().rename(\"cost_med\")\n",
    "\n",
    "ref = (anchor.merge(hist, on=\"product\", how=\"left\")\n",
    "             .merge(p_med, on=\"product\", how=\"left\")\n",
    "             .merge(c_med, on=\"product\", how=\"left\"))\n",
    "\n",
    "ref[\"PCR\"]     = safe_div(ref[\"p_anchor\"], ref[\"supplier_price_ref\"])\n",
    "ref[\"PCR_med\"] = safe_div(ref[\"price_med\"], ref[\"cost_med\"])\n",
    "\n",
    "# Risk score: higher → more cautious amounts\n",
    "w1, w2, w3 = 0.5, 0.3, 0.2\n",
    "VOL_med = ref[\"VOL\"].median() if ref[\"VOL\"].median() > 0 else 1.0\n",
    "ref[\"RISK\"] = (\n",
    "    (ref[\"PCR\"] / ref[\"PCR_med\"]).replace([np.inf, -np.inf], np.nan).fillna(1.0)*w1 +\n",
    "    (ref[\"p_anchor\"] / ref[\"price_med\"]).replace([np.inf, -np.inf], np.nan).fillna(1.0)*w2 +\n",
    "    (ref[\"VOL\"] / VOL_med).replace([np.inf, -np.inf], np.nan).fillna(1.0)*w3\n",
    ")\n",
    "\n",
    "# Dead items: no sales in last 2 historical weeks (if exist)\n",
    "last_weeks = sorted(HIST_VERS)[-2:] if len(HIST_VERS) >= 2 else HIST_VERS\n",
    "recent = g_hist[g_hist[\"version\"].isin(last_weeks)].groupby(\"product\")[\"qty\"].sum()\n",
    "ref = ref.merge(recent.rename(\"qty_recent\"), on=\"product\", how=\"left\").fillna({\"qty_recent\":0.0})\n",
    "ref[\"is_dead\"] = ref[\"qty_recent\"] <= 0\n",
    "\n",
    "# Recency: weeks since last non-zero sale across all versions\n",
    "last_sale_v = (g[g[\"qty\"]>0].groupby(\"product\")[\"version\"].max().rename(\"last_sold_version\"))\n",
    "ref = ref.merge(last_sale_v, on=\"product\", how=\"left\")\n",
    "ref[\"weeks_since_sale\"]   = V_NOW - ref[\"last_sold_version\"].fillna(0)\n",
    "ref[\"recency_multiplier\"] = (1.0 - RECENCY_DECAY*ref[\"weeks_since_sale\"]).clip(lower=RECENCY_MIN, upper=1.0)\n",
    "\n",
    "print(\"Indicators sample:\")\n",
    "display(ref.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab3d77b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Price rule (anchored + TAST/recency/dead guards) ========================\n",
    "def price_next(r):\n",
    "    p0   = float(r[\"p_anchor\"] or 0.0)\n",
    "    sp   = float(r[\"supplier_price_ref\"] or 0.0)\n",
    "    tast = float(r[\"TAST_EMA\"] or 0.0)   # time-aware demand\n",
    "    dead = bool(r[\"is_dead\"])\n",
    "    rec  = float(r[\"recency_multiplier\"] or 1.0)\n",
    "\n",
    "    # anchored default\n",
    "    p = p0\n",
    "    if dead:\n",
    "        p = p0 * (1.0 - PRICE_DOWN_DEAD)\n",
    "    elif tast >= TAST_HOT:\n",
    "        p = p0 * (1.0 + PRICE_UP_HOT)\n",
    "    elif TAST_GOOD_LOW <= tast < TAST_HOT:\n",
    "        p = p0 * (1.0 + PRICE_UP_GOOD)\n",
    "    elif tast <= TAST_WEAK_HIGH and (r[\"amount_avg\"] or 0) > 0:\n",
    "        p = p0 * (1.0 - PRICE_DOWN_WEAK)\n",
    "\n",
    "    # Recency decay (penalize stale items)\n",
    "    p *= rec\n",
    "\n",
    "    # Margin floor\n",
    "    min_allowed = sp*(1.0 + MIN_MARGIN_RATE) if sp>0 else 0.0\n",
    "    p = max(p, min_allowed)\n",
    "\n",
    "    # Per-round caps (tighter for living items, looser for dead)\n",
    "    cap_up   = p0*(1.0 + (ROUND_CAP_DEAD if dead else ROUND_CAP_STD))\n",
    "    cap_down = p0*(1.0 - (ROUND_CAP_DEAD if dead else ROUND_CAP_STD))\n",
    "    p = min(max(p, cap_down), cap_up)\n",
    "\n",
    "    return round(float(p), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543162f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Amount rule (TAST + risk cap + recency/dead) ============================\n",
    "def amount_next(r):\n",
    "    tast    = float(r[\"TAST_EMA\"] or 0.0)\n",
    "    qty_max = float(r[\"qty_max\"] or 0.0)\n",
    "    amt_avg = float(r[\"amount_avg\"] or 0.0)\n",
    "    risk    = float(r[\"RISK\"] or 1.0)\n",
    "    dead    = bool(r[\"is_dead\"])\n",
    "    rec     = float(r[\"recency_multiplier\"] or 1.0)\n",
    "\n",
    "    # Base: over observed max demand\n",
    "    base = math.ceil(qty_max * AMT_BASE_BUFFER)\n",
    "    if tast > 0.95:\n",
    "        base = math.ceil(base * AMT_BOOST_HOT)\n",
    "    if dead:\n",
    "        base = max(0, round(amt_avg * AMT_DEAD_FACTOR))\n",
    "\n",
    "    # Risk cap: higher risk => smaller cap multiplier\n",
    "    cap_mult = np.clip(1.0 + 0.5/(risk if risk>0 else 1.0), RISK_CAP_MIN, RISK_CAP_MAX)\n",
    "    cap = amt_avg * cap_mult\n",
    "    if cap > 0:\n",
    "        base = min(base, math.ceil(cap))\n",
    "\n",
    "    # Recency downscale\n",
    "    base = int(base * rec)\n",
    "\n",
    "    return int(max(0, base))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d100ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Schedule selection by ROI ==============================================\n",
    "# Use md: version-level revenue and staff cost (avoid double-count: take one staff cost per version)\n",
    "rev_by_v   = md.groupby(\"version\")[\"revenue\"].sum()\n",
    "staff_by_v = md.groupby(\"version\")[\"version_staff_cost\"].max()  # single value per version\n",
    "roi = (rev_by_v / staff_by_v.replace(0, np.nan)).dropna()\n",
    "\n",
    "# Consider only historical weeks\n",
    "roi_hist = roi[roi.index.isin(HIST_VERS)]\n",
    "if not roi_hist.empty:\n",
    "    sched_src_v = int(roi_hist.idxmax())\n",
    "else:\n",
    "    # fallback to latest historical with a schedule file\n",
    "    candidates = [v for v in reversed(HIST_VERS) if (SCHEDULES_DIR / f\"schedules_{v}.json\").exists()]\n",
    "    sched_src_v = candidates[0] if candidates else HIST_VERS[-1]\n",
    "\n",
    "with open(SCHEDULES_DIR / f\"schedules_{sched_src_v}.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    schedules_next = json.load(f)\n",
    "\n",
    "print(f\"Schedule source version (best ROI among history): v{sched_src_v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84121f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate JSONs & Audit ==================================================\n",
    "prices_next  = {r[\"product\"]: price_next(r)  for _, r in ref.iterrows()}\n",
    "amounts_next = {r[\"product\"]: amount_next(r) for _, r in ref.iterrows()}\n",
    "\n",
    "# Write decisions\n",
    "(DATA_DIR/\"prices\").mkdir(parents=True, exist_ok=True)\n",
    "(DATA_DIR/\"amounts\").mkdir(parents=True, exist_ok=True)\n",
    "(DATA_DIR/\"schedules\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(DATA_DIR/\"prices\"/f\"prices_{V_NEXT}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(prices_next, f, indent=2, ensure_ascii=False)\n",
    "with open(DATA_DIR/\"amounts\"/f\"amounts_{V_NEXT}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(amounts_next, f, indent=2, ensure_ascii=False)\n",
    "with open(DATA_DIR/\"schedules\"/f\"schedules_{V_NEXT}.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(schedules_next, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Audit table\n",
    "audit = ref[[\"product\",\"anchor_v\",\"p_anchor\",\"qty_max\",\"amount_avg\",\"TAST_EMA\",\"RISK\",\"is_dead\",\"weeks_since_sale\",\"recency_multiplier\"]].copy()\n",
    "audit[\"price_next\"]  = audit[\"product\"].map(prices_next)\n",
    "audit[\"amount_next\"] = audit[\"product\"].map(amounts_next)\n",
    "audit[\"d_price_vs_anchor_%\"] = (audit[\"price_next\"] - audit[\"p_anchor\"]) / audit[\"p_anchor\"] * 100\n",
    "\n",
    "audit_path = OUT_DIR / f\"decision_v{V_NEXT}_audit.csv\"\n",
    "audit.to_csv(audit_path, index=False)\n",
    "\n",
    "print(\"Generated decisions:\")\n",
    "print(\" -\", DATA_DIR/\"prices\"/f\"prices_{V_NEXT}.json\")\n",
    "print(\" -\", DATA_DIR/\"amounts\"/f\"amounts_{V_NEXT}.json\")\n",
    "print(\" -\", DATA_DIR/\"schedules\"/f\"schedules_{V_NEXT}.json\")\n",
    "print(\"Audit:\", audit_path)\n",
    "\n",
    "display(audit.sort_values([\"is_dead\",\"weeks_since_sale\",\"RISK\"], ascending=[False, False, False]).head(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50fec89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Quick anomaly checks ====================================================\n",
    "def show_anomalies(filters=ANOMALY_FILTERS, df=audit):\n",
    "    if not filters:\n",
    "        print(\"No filters provided.\"); return\n",
    "    flt = None\n",
    "    for token in filters:\n",
    "        m = df[\"product\"].str.contains(token, case=False, na=False)\n",
    "        flt = m if flt is None else (flt | m)\n",
    "    res = df[flt] if flt is not None else pd.DataFrame()\n",
    "    if res.empty:\n",
    "        print(\"No products matched filters:\", filters)\n",
    "    else:\n",
    "        print(\"Anomaly candidates:\")\n",
    "        display(res.sort_values([\"is_dead\",\"weeks_since_sale\",\"RISK\"], ascending=[False, False, False]))\n",
    "\n",
    "show_anomalies()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6508bd64",
   "metadata": {},
   "source": [
    "### Notes & Rationale\n",
    "\n",
    "- **TAST_norm** (time-adjusted sell-through) replaces raw sell-through as primary demand signal.\n",
    "- **Anchor pricing** avoids using stale or depressed last-week prices; we anchor to the best-selling week.\n",
    "- **Recency decay** reduces both price and amount for items with a long time since last sale.\n",
    "- **Dead-item suppression** applies stronger reductions and lower restock targets.\n",
    "- **Risk cap** prevents big stock increases on expensive or volatile items.\n",
    "- **Schedule ROI** picks the historical week with the best revenue/staff-cost ratio.\n",
    "\n",
    "Per-round caps keep changes conservative in a competitive environment.\n",
    "If any product looks odd in the audit, tune:\n",
    "- `RECENCY_DECAY` (0.05–0.15), `RECENCY_MIN` (0.5–0.8),\n",
    "- amount caps (`RISK_CAP_MIN/MAX`),\n",
    "- price caps (`ROUND_CAP_STD/DEAD`),\n",
    "- TAST thresholds (`TAST_HOT`, `TAST_GOOD_LOW`, `TAST_WEAK_HIGH`)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
